{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# READ ME\n",
        "\n",
        "1. The runtime should be L4 GPU.\n",
        "2. Just run all the cells by clicking Runtime -> Run all.\n",
        "3. To test on celebA data, upload images from celebA_data (from github) and the model will generate captions."
      ],
      "metadata": {
        "id": "Qzf29ufWgESW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation\n",
        "\n",
        "1. The packages are loaded.\n",
        "2. The data is loaded.\n",
        "3. The model and the processor are loaded.\n",
        "4. The data has only train so the train is split into train, val and test.\n",
        "5. The data is processed to fine tune.\n",
        "6. The processed data is fine tuned on train data and evaluated on val data for 4 epochs.\n",
        "7. The model is saved in a .zip format.\n",
        "8. The model is evaluated on test data and last 4 predicted captions are printed along with ground truth caption and image.\n",
        "9. The model then takes 4 input images of celebA uploaded to the colab folder.\n",
        "10. Captions are generated for the images."
      ],
      "metadata": {
        "id": "BOnQiZTdgHDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "VoLDYGvJgwX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGDo9A63c_QG",
        "outputId": "afff576d-29b7-45ea-d86a-377406b04d1f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from transformers import BlipForConditionalGeneration, BlipProcessor, AdamW"
      ],
      "metadata": {
        "id": "Y9tdGznplF_y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "EpEpskMigyrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data, model and processor"
      ],
      "metadata": {
        "id": "JKuLJo8qg4Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CelebA Dataset\n",
        "class CelebALlavaDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        \"\"\"\n",
        "        Dataset for CelebA with LLaVA captions.\n",
        "        Args:\n",
        "            dataset: Subset of Hugging Face dataset containing images and text captions.\n",
        "            processor: BLIP2 processor for tokenizing text and processing images.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataset[idx]\n",
        "        image = data['image']\n",
        "        caption = data['text']\n",
        "\n",
        "        # Process the image\n",
        "        image_encoding = self.processor.image_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        # Process the caption\n",
        "        text_encoding = self.processor.tokenizer(\n",
        "            caption,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=64\n",
        "        )\n",
        "\n",
        "        encoding = {\n",
        "            \"pixel_values\": image_encoding[\"pixel_values\"].squeeze(0),\n",
        "            \"input_ids\": text_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": text_encoding[\"attention_mask\"].squeeze(0),\n",
        "        }\n",
        "\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "sl7PEjCxkvKE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset and Processor\n",
        "dataset = load_dataset(\"irodkin/celeba_with_llava_captions\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0az_sWpk_AA",
        "outputId": "e522b3f3-472e-4440-c3f6-3ae70842b27b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling from the original data"
      ],
      "metadata": {
        "id": "jb5Y10_LhFSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = 4000  # Number of samples to use\n",
        "indices = list(range(len(dataset['train'])))\n",
        "subset_indices = indices[:subset_size]\n",
        "subset = Subset(dataset['train'], subset_indices)"
      ],
      "metadata": {
        "id": "Zlt1Sl5Bk7N_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the subset into training (70%), validation (15%), and test (15%)\n",
        "train_size = int(0.7 * len(subset))\n",
        "val_size = int(0.15 * len(subset))\n",
        "test_size = len(subset) - train_size - val_size"
      ],
      "metadata": {
        "id": "vWMNZYbUk5lq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = random_split(subset, [train_size, val_size, test_size])"
      ],
      "metadata": {
        "id": "Qq5XiQpck3iA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets\n",
        "train_dataset = CelebALlavaDataset(train_data, processor)\n",
        "val_dataset = CelebALlavaDataset(val_data, processor)\n",
        "test_dataset = CelebALlavaDataset(test_data, processor)"
      ],
      "metadata": {
        "id": "07_WoA8nk1uo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 8\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "0s3HDRyYk0FV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "Xq6Fha8BhK6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BLIP Image Caption Model\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utLdv0iWkyAp",
        "outputId": "22d04a44-19ad-43d3-8102-6fc7afb620e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BlipForConditionalGeneration(\n",
              "  (vision_model): BlipVisionModel(\n",
              "    (embeddings): BlipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (encoder): BlipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BlipEncoderLayer(\n",
              "          (self_attn): BlipAttention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): BlipMLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (text_decoder): BlipTextLMHeadModel(\n",
              "    (bert): BlipTextModel(\n",
              "      (embeddings): BlipTextEmbeddings(\n",
              "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (encoder): BlipTextEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BlipTextLayer(\n",
              "            (attention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BlipTextAttention(\n",
              "              (self): BlipTextSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "              (output): BlipTextSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BlipTextIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BlipTextOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BlipTextOnlyMLMHead(\n",
              "      (predictions): BlipTextLMPredictionHead(\n",
              "        (transform): BlipTextPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and Training Configuration\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "epochs = 4"
      ],
      "metadata": {
        "id": "aVUmV1Emkwyj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Validation Loop\n",
        "for epoch in range(epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    prog_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch: {epoch+1}\")\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        prog_bar.update(1)\n",
        "\n",
        "        # Move data to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    prog_bar.close()\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {train_loss}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                pixel_values=pixel_values,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=input_ids\n",
        "            )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WOB-CCmkhFL",
        "outputId": "6e86b5fd-b85d-48bf-f195-4ba01498e8ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1: 100%|██████████| 350/350 [03:51<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.4052765366009303\n",
            "Epoch 1, Validation Loss: 0.788023070494334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2: 100%|██████████| 350/350 [03:50<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 0.6470922005176544\n",
            "Epoch 2, Validation Loss: 0.5976575712362925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3: 100%|██████████| 350/350 [03:50<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 0.5235537646497999\n",
            "Epoch 3, Validation Loss: 0.558081267674764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4: 100%|██████████| 350/350 [03:50<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 0.45805561465876443\n",
            "Epoch 4, Validation Loss: 0.5412570563952128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fine tuning complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xQcBmHYwDVC",
        "outputId": "56e11e3a-9865-44bd-fa7b-321756a0d4aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine tuning complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing"
      ],
      "metadata": {
        "id": "yKt4vLoqhOzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Phase\n",
        "print(\"Evaluating on test data...\")\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "num_examples_to_print = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6SnTOWSvcY8",
        "outputId": "11de9e00-9a33-4a14-8c75-de2992754518"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions = []\n",
        "all_references = []"
      ],
      "metadata": {
        "id": "Iw0SDdZNvens"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(test_dataloader):\n",
        "        print(f\"Batch {batch_idx} processing\")\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Compute loss\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "        test_loss += outputs.loss.item()\n",
        "\n",
        "        # Generate predictions\n",
        "        generated_outputs = model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            max_length=64,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode predictions and references\n",
        "        predictions = [\n",
        "            processor.tokenizer.decode(output, skip_special_tokens=True)\n",
        "            for output in generated_outputs\n",
        "        ]\n",
        "        references = [\n",
        "            processor.tokenizer.decode(ref, skip_special_tokens=True)\n",
        "            for ref in input_ids\n",
        "        ]\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "        all_references.extend(references)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcrS2sf3kIxL",
        "outputId": "8351de95-5643-4d8f-a340-660ade62961f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 processing\n",
            "Batch 1 processing\n",
            "Batch 2 processing\n",
            "Batch 3 processing\n",
            "Batch 4 processing\n",
            "Batch 5 processing\n",
            "Batch 6 processing\n",
            "Batch 7 processing\n",
            "Batch 8 processing\n",
            "Batch 9 processing\n",
            "Batch 10 processing\n",
            "Batch 11 processing\n",
            "Batch 12 processing\n",
            "Batch 13 processing\n",
            "Batch 14 processing\n",
            "Batch 15 processing\n",
            "Batch 16 processing\n",
            "Batch 17 processing\n",
            "Batch 18 processing\n",
            "Batch 19 processing\n",
            "Batch 20 processing\n",
            "Batch 21 processing\n",
            "Batch 22 processing\n",
            "Batch 23 processing\n",
            "Batch 24 processing\n",
            "Batch 25 processing\n",
            "Batch 26 processing\n",
            "Batch 27 processing\n",
            "Batch 28 processing\n",
            "Batch 29 processing\n",
            "Batch 30 processing\n",
            "Batch 31 processing\n",
            "Batch 32 processing\n",
            "Batch 33 processing\n",
            "Batch 34 processing\n",
            "Batch 35 processing\n",
            "Batch 36 processing\n",
            "Batch 37 processing\n",
            "Batch 38 processing\n",
            "Batch 39 processing\n",
            "Batch 40 processing\n",
            "Batch 41 processing\n",
            "Batch 42 processing\n",
            "Batch 43 processing\n",
            "Batch 44 processing\n",
            "Batch 45 processing\n",
            "Batch 46 processing\n",
            "Batch 47 processing\n",
            "Batch 48 processing\n",
            "Batch 49 processing\n",
            "Batch 50 processing\n",
            "Batch 51 processing\n",
            "Batch 52 processing\n",
            "Batch 53 processing\n",
            "Batch 54 processing\n",
            "Batch 55 processing\n",
            "Batch 56 processing\n",
            "Batch 57 processing\n",
            "Batch 58 processing\n",
            "Batch 59 processing\n",
            "Batch 60 processing\n",
            "Batch 61 processing\n",
            "Batch 62 processing\n",
            "Batch 63 processing\n",
            "Batch 64 processing\n",
            "Batch 65 processing\n",
            "Batch 66 processing\n",
            "Batch 67 processing\n",
            "Batch 68 processing\n",
            "Batch 69 processing\n",
            "Batch 70 processing\n",
            "Batch 71 processing\n",
            "Batch 72 processing\n",
            "Batch 73 processing\n",
            "Batch 74 processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average test loss\n",
        "test_loss /= len(test_dataloader)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "2ggVPEfwvm6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1f320c-698b-4564-849d-19cd73cf73b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5668295645713806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the last few predictions and references\n",
        "print(\"\\nLast few predictions:\")\n",
        "for idx in range(-num_examples_to_print, 0):\n",
        "    print(f\"Example {len(all_predictions) + idx}\")\n",
        "    print(f\"Prediction: {all_predictions[idx]}\")\n",
        "    print(f\"Reference: {all_references[idx]}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "KkpFr3UUvkv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b77ea4-62da-4ae8-b4e7-e65667ae0d4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Last few predictions:\n",
            "Example 596\n",
            "Prediction: the person in the image is a man with a beard, wearing a suit and tie. he has a smile on his face, and his eyes are described as being small and close together. the man ' s nose is described as being small, and his mouth is described as being wide. the man ' s\n",
            "Reference: the person in the image is a young man with a smile on his face. he has a small nose, a thin mouth, and a wide smile. his eyes are brown and are described as being very large. the young man is wearing glasses and has a beard. he is also wearing a suit, which\n",
            "------------------------------\n",
            "Example 597\n",
            "Prediction: the person in the image is a young woman with long, curly hair. she has a heart - shaped face, a small nose, and a smile on her face. her eyes are large and brown, and she is wearing glasses. her hair is blonde, and she is wearing a white shirt. the woman\n",
            "Reference: the person in the image is a beautiful young woman with long, curly hair. she is wearing a white bikini top and smiling, which suggests that she is in a beach setting. the woman has a heart - shaped face, a small nose, and a wide smile. her eyes are brown and her hair is\n",
            "------------------------------\n",
            "Example 598\n",
            "Prediction: the person in the image is a woman, and she is wearing a suit and tie. her eye shape is described as being large, and she has a small nose. her nose is narrow, and her lips are thick. her facial shape is described as being wide, and her hair is blonde. the woman\n",
            "Reference: the person in the image is a woman with a round face, a small nose, and a thin mouth. she has a thin, straight eyebrow, and her eyes are large and brown. her hair is dark, and she is wearing a suit and tie. the woman appears to be an adult, and she\n",
            "------------------------------\n",
            "Example 599\n",
            "Prediction: the person in the image is a young woman with long, curly hair. she has a heart - shaped face, a small nose, and a thin, straight mouth. her eyes are large and brown, and she is wearing a flower in her hair style. the woman is also wearing a necklace and has a\n",
            "Reference: the person in the image is a beautiful young woman with long hair. she has a heart - shaped face, large eyes, and a prominent nose. the woman is wearing a flower in her hair, and her eyes are described as being very pretty. she is also wearing a necklace, which adds to her overall\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "uVgpnlhJhRxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and processor\n",
        "project_path = \"./fine_tuned_blip_base_celeba\"\n",
        "model.save_pretrained(project_path)\n",
        "processor.save_pretrained(project_path)"
      ],
      "metadata": {
        "id": "uZlBQDPKkj_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4c106e6-ed58-4613-e462-a9eaf0a0a82b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to ZIP\n",
        "zip_path = \"./fine_tuned_blip_base_celeba.zip\"\n",
        "shutil.make_archive(base_name=project_path, format='zip', root_dir=project_path)\n",
        "print(f\"Model and processor exported to {zip_path}\")"
      ],
      "metadata": {
        "id": "7NrboVMNmztI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d159297-97aa-420a-dbab-c2ac8ef74ecf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and processor exported to ./fine_tuned_blip_base_celeba.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model on input image"
      ],
      "metadata": {
        "id": "_kleE8-Mgo_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_random_image(image_path, model, processor, device):\n",
        "    \"\"\"\n",
        "    Test the fine tuned model on a random image and generate a caption.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file.\n",
        "        model: Fine tuned model.\n",
        "        processor: Processor (e.g., BlipProcessor) for preprocessing the image.\n",
        "        device: Device to run the model on (e.g., \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "        str: Generated caption for the image.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = processor.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    # Generate prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            max_length=64,\n",
        "            num_beams=5,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated caption\n",
        "    caption = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return caption"
      ],
      "metadata": {
        "id": "45nKbWI0vHxO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_path = \"image_1.jpg\"\n",
        "ground_truth = \"The person in the image is a beautiful young woman with long, curly hair. She has a heart-shaped face, large eyes, and a small nose. Her eyes are described as being very pretty, and she is wearing a necklace. The woman is also described as a young adult, which suggests that she is likely in her late teens or early twenties.\"\n",
        "caption = test_random_image(random_image_path, model, processor, device)\n",
        "print(f\"Ground Truth Caption: {ground_truth}\")\n",
        "print(f\"Generated Caption: {caption}\")"
      ],
      "metadata": {
        "id": "kMWNS-5f4toV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e0118b-45c9-4c90-c0d0-69021d4710b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth Caption: The person in the image is a beautiful young woman with long, curly hair. She has a heart-shaped face, large eyes, and a small nose. Her eyes are described as being very pretty, and she is wearing a necklace. The woman is also described as a young adult, which suggests that she is likely in her late teens or early twenties.\n",
            "Generated Caption: the person in the image is a young woman with long, curly hair. she has a heart - shaped face, a small nose, and a smile. her eyes are large and expressive, and she is wearing glasses. her hair is dark, and she is wearing a necklace. the woman is described as a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_path = \"image_2.jpg\"\n",
        "ground_truth = \"The person in the image is a young man with a beard, wearing a blue shirt. He has a round face, a small nose, and a thin mouth. His eyes are large and round, and he has a smile on his face. The man is wearing glasses, which suggests that he may have vision issues or simply prefers wearing them for style. The image shows that he is a young adult, possibly a teenager or a young man.\"\n",
        "caption = test_random_image(random_image_path, model, processor, device)\n",
        "print(f\"Ground Truth Caption: {ground_truth}\")\n",
        "print(f\"Generated Caption: {caption}\")"
      ],
      "metadata": {
        "id": "Na4Spfiz4xJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb15df54-6374-451b-d123-9796f6b114cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth Caption: The person in the image is a young man with a beard, wearing a blue shirt. He has a round face, a small nose, and a thin mouth. His eyes are large and round, and he has a smile on his face. The man is wearing glasses, which suggests that he may have vision issues or simply prefers wearing them for style. The image shows that he is a young adult, possibly a teenager or a young man.\n",
            "Generated Caption: the person in the image is a young man with a beard, wearing glasses. he has a round face, a small nose, and a thin mouth. his eyes are large and brown, and he is wearing a black shirt. the image also shows a close - up of the man ' s face, which\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_path = \"image_3.jpg\"\n",
        "ground_truth = \"The person in the image is a woman with a smile on her face. She has a large nose, and her eyes are shaped like a cat's. Her nose is wide, and her eyes are brown. She is wearing a scarf around her neck, and her hair is blonde. The woman is described as a beautiful woman, which suggests that she might be a young adult or an adult. The image does not provide enough information to determine her age, race, or gender.\"\n",
        "caption = test_random_image(random_image_path, model, processor, device)\n",
        "print(f\"Ground Truth Caption: {ground_truth}\")\n",
        "print(f\"Generated Caption: {caption}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zeDY77FjE91",
        "outputId": "99a9244b-2ae1-41b3-d82c-d9d57a761b97"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth Caption: The person in the image is a woman with a smile on her face. She has a large nose, and her eyes are shaped like a cat's. Her nose is wide, and her eyes are brown. She is wearing a scarf around her neck, and her hair is blonde. The woman is described as a beautiful woman, which suggests that she might be a young adult or an adult. The image does not provide enough information to determine her age, race, or gender.\n",
            "Generated Caption: the person in the image is a woman, and she is wearing a scarf around her neck. her eye shape is described as being large, and she has a smile on her face. her nose is described as being small, and her lips are described as being thick. her facial shape is described as being wide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_image_path = \"image_4.jpg\"\n",
        "ground_truth = \"The person in the image is a young man, likely a teenager or young adult, with a smiling expression. He has a small nose, thin lips, and a wide mouth. His facial shape is oval, and his eyes are large and blue. He is wearing a blue shirt and a helmet, which suggests that he is a racing driver. The image does not provide enough information to determine his race, gender, or age.\"\n",
        "caption = test_random_image(random_image_path, model, processor, device)\n",
        "print(f\"Ground Truth Caption: {ground_truth}\")\n",
        "print(f\"Generated Caption: {caption}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQAqiH-vjFcu",
        "outputId": "0209f060-71d0-4dd9-ff3e-3cd91c8d5774"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth Caption: The person in the image is a young man, likely a teenager or young adult, with a smiling expression. He has a small nose, thin lips, and a wide mouth. His facial shape is oval, and his eyes are large and blue. He is wearing a blue shirt and a helmet, which suggests that he is a racing driver. The image does not provide enough information to determine his race, gender, or age.\n",
            "Generated Caption: the person in the image is a young man, likely a teenager or young adult, with a smiling expression. he has a round face, a small nose, and a thin mouth. his eyes are large and expressive, and he is wearing a blue racing suit. the image also shows a close - up of\n"
          ]
        }
      ]
    }
  ]
}